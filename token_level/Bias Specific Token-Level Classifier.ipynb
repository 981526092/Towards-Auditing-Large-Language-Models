{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be6b35f9-9baa-485e-9c13-f4603700d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DataCollatorForTokenClassification,pipeline,AutoModelForTokenClassification,AutoTokenizer, Trainer, TrainingArguments,DataCollatorWithPadding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import re\n",
    "import evaluate\n",
    "\n",
    "from training import load_data,prepare_dataset,prepare_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "model_path = \"distilbert-base-uncased\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset stereoset (/Users/zekunwu/.cache/huggingface/datasets/stereoset/intrasentence/1.0.0/b188e395e95b37c7a095ebc2de352fbdb249d67d1beb2ff639bb4dc37dfbb090)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f07820cf92584fe7b9f5710d82f912e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "intrasentence_dataset = load_data(\"intrasentence\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bfd9633-5ed3-4c0f-bbd6-74996e8d2242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2430,)\n",
      "(2886,)\n",
      "(765,)\n",
      "(237,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(intrasentence_dataset[\"profession\"]))\n",
    "print(np.shape(intrasentence_dataset[\"race\"]))\n",
    "print(np.shape(intrasentence_dataset[\"gender\"]))\n",
    "print(np.shape(intrasentence_dataset[\"religion\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b166455-f4da-403f-b6cb-9ef50e35dd7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2886 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f23351bcca624315817275e041e6dfa8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['tokens', 'labels', 'input_ids', 'attention_mask'],\n        num_rows: 2308\n    })\n    test: Dataset({\n        features: ['tokens', 'labels', 'input_ids', 'attention_mask'],\n        num_rows: 578\n    })\n})"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "data = prepare_text(intrasentence_dataset[\"race\"])\n",
    "tokenized_data = prepare_dataset(tokenizer,data)\n",
    "final_dataset = tokenized_data.train_test_split(0.2)\n",
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b8ef322-ae48-4ae8-846d-2d2a80932418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data[17][\"tokens\"])\n",
    "# tokenized_input = tokenizer(data[17][\"tokens\"], is_split_into_words=True)\n",
    "# tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c58e619-c0d9-45d3-afc4-129af2ce83ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[codecarbon INFO @ 21:14:03] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 21:14:03] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 21:14:03] No GPU found.\n",
      "[codecarbon INFO @ 21:14:03] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 21:14:03] No CPU tracking mode found. Falling back on CPU constant mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 21:14:04] We saw that you have a Apple M2 but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 21:14:04] CPU Model on constant consumption mode: Apple M2\n",
      "[codecarbon INFO @ 21:14:04] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 21:14:04]   Platform system: macOS-13.0.1-arm64-arm-64bit\n",
      "[codecarbon INFO @ 21:14:04]   Python version: 3.9.6\n",
      "[codecarbon INFO @ 21:14:04]   CodeCarbon version: 2.2.3\n",
      "[codecarbon INFO @ 21:14:04]   Available RAM : 8.000 GB\n",
      "[codecarbon INFO @ 21:14:04]   CPU count: 8\n",
      "[codecarbon INFO @ 21:14:04]   CPU model: Apple M2\n",
      "[codecarbon INFO @ 21:14:04]   GPU count: None\n",
      "[codecarbon INFO @ 21:14:04]   GPU model: None\n",
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens.\n",
      "/Users/zekunwu/Library/Python/3.9/lib/python/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2308\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 870\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/870 : < :, Epoch 0.01/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 21:14:22] Energy consumed for RAM : 0.000013 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:14:22] Energy consumed for all CPUs : 0.000177 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:14:22] 0.000190 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:14:37] Energy consumed for RAM : 0.000025 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:14:37] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:14:37] 0.000379 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:14:52] Energy consumed for RAM : 0.000038 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:14:52] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:14:52] 0.000569 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 578\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to specific_best_model/race/checkpoint-145\n",
      "Configuration saved in specific_best_model/race/checkpoint-145/config.json\n",
      "Model weights saved in specific_best_model/race/checkpoint-145/pytorch_model.bin\n",
      "tokenizer config file saved in specific_best_model/race/checkpoint-145/tokenizer_config.json\n",
      "Special tokens file saved in specific_best_model/race/checkpoint-145/special_tokens_map.json\n",
      "Deleting older checkpoint [specific_best_model/race/checkpoint-435] due to args.save_total_limit\n",
      "Deleting older checkpoint [specific_best_model/race/checkpoint-870] due to args.save_total_limit\n",
      "[codecarbon INFO @ 21:15:07] Energy consumed for RAM : 0.000050 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:15:07] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:15:07] 0.000759 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:15:22] Energy consumed for RAM : 0.000063 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:15:22] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:15:22] 0.000948 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:15:37] Energy consumed for RAM : 0.000075 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:15:37] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:15:37] 0.001138 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 578\n",
      "  Batch size = 16\n",
      "[codecarbon INFO @ 21:15:52] Energy consumed for RAM : 0.000087 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:15:52] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:15:52] 0.001327 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to specific_best_model/race/checkpoint-290\n",
      "Configuration saved in specific_best_model/race/checkpoint-290/config.json\n",
      "Model weights saved in specific_best_model/race/checkpoint-290/pytorch_model.bin\n",
      "tokenizer config file saved in specific_best_model/race/checkpoint-290/tokenizer_config.json\n",
      "Special tokens file saved in specific_best_model/race/checkpoint-290/special_tokens_map.json\n",
      "Deleting older checkpoint [specific_best_model/race/checkpoint-145] due to args.save_total_limit\n",
      "[codecarbon INFO @ 21:16:07] Energy consumed for RAM : 0.000100 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:16:07] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:16:07] 0.001517 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:16:22] Energy consumed for RAM : 0.000113 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:16:22] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:16:22] 0.001707 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:16:37] Energy consumed for RAM : 0.000125 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:16:37] Energy consumed for all CPUs : 0.001771 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:16:37] 0.001896 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 578\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to specific_best_model/race/checkpoint-435\n",
      "Configuration saved in specific_best_model/race/checkpoint-435/config.json\n",
      "Model weights saved in specific_best_model/race/checkpoint-435/pytorch_model.bin\n",
      "tokenizer config file saved in specific_best_model/race/checkpoint-435/tokenizer_config.json\n",
      "Special tokens file saved in specific_best_model/race/checkpoint-435/special_tokens_map.json\n",
      "[codecarbon INFO @ 21:16:52] Energy consumed for RAM : 0.000138 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:16:52] Energy consumed for all CPUs : 0.001948 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:16:52] 0.002086 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:17:07] Energy consumed for RAM : 0.000150 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:17:07] Energy consumed for all CPUs : 0.002125 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:17:07] 0.002275 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:17:22] Energy consumed for RAM : 0.000163 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:17:22] Energy consumed for all CPUs : 0.002303 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:17:22] 0.002465 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 578\n",
      "  Batch size = 16\n",
      "[codecarbon INFO @ 21:17:37] Energy consumed for RAM : 0.000175 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:17:37] Energy consumed for all CPUs : 0.002480 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:17:37] 0.002655 kWh of electricity used since the beginning.\n",
      "Saving model checkpoint to specific_best_model/race/checkpoint-580\n",
      "Configuration saved in specific_best_model/race/checkpoint-580/config.json\n",
      "Model weights saved in specific_best_model/race/checkpoint-580/pytorch_model.bin\n",
      "tokenizer config file saved in specific_best_model/race/checkpoint-580/tokenizer_config.json\n",
      "Special tokens file saved in specific_best_model/race/checkpoint-580/special_tokens_map.json\n",
      "Deleting older checkpoint [specific_best_model/race/checkpoint-435] due to args.save_total_limit\n",
      "[codecarbon INFO @ 21:17:52] Energy consumed for RAM : 0.000188 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:17:52] Energy consumed for all CPUs : 0.002657 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:17:52] 0.002844 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:18:07] Energy consumed for RAM : 0.000200 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:18:07] Energy consumed for all CPUs : 0.002834 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:18:07] 0.003034 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:18:22] Energy consumed for RAM : 0.000213 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:18:22] Energy consumed for all CPUs : 0.003011 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:18:22] 0.003224 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 578\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to specific_best_model/race/checkpoint-725\n",
      "Configuration saved in specific_best_model/race/checkpoint-725/config.json\n",
      "Model weights saved in specific_best_model/race/checkpoint-725/pytorch_model.bin\n",
      "tokenizer config file saved in specific_best_model/race/checkpoint-725/tokenizer_config.json\n",
      "Special tokens file saved in specific_best_model/race/checkpoint-725/special_tokens_map.json\n",
      "Deleting older checkpoint [specific_best_model/race/checkpoint-580] due to args.save_total_limit\n",
      "[codecarbon INFO @ 21:18:37] Energy consumed for RAM : 0.000225 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:18:37] Energy consumed for all CPUs : 0.003188 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:18:37] 0.003413 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:18:52] Energy consumed for RAM : 0.000238 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:18:52] Energy consumed for all CPUs : 0.003365 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:18:52] 0.003603 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:19:07] Energy consumed for RAM : 0.000250 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:19:07] Energy consumed for all CPUs : 0.003542 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:19:07] 0.003792 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 21:19:22] Energy consumed for RAM : 0.000263 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:19:22] Energy consumed for all CPUs : 0.003720 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:19:22] 0.003982 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 578\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to specific_best_model/race/checkpoint-870\n",
      "Configuration saved in specific_best_model/race/checkpoint-870/config.json\n",
      "Model weights saved in specific_best_model/race/checkpoint-870/pytorch_model.bin\n",
      "tokenizer config file saved in specific_best_model/race/checkpoint-870/tokenizer_config.json\n",
      "Special tokens file saved in specific_best_model/race/checkpoint-870/special_tokens_map.json\n",
      "Deleting older checkpoint [specific_best_model/race/checkpoint-725] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from specific_best_model/race/checkpoint-290 (score: 0.08452929556369781).\n",
      "[codecarbon INFO @ 21:19:31] Energy consumed for RAM : 0.000270 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 21:19:31] Energy consumed for all CPUs : 0.003828 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 21:19:31] 0.004098 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=870, training_loss=0.06965746057444605, metrics={'train_runtime': 324.2798, 'train_samples_per_second': 42.704, 'train_steps_per_second': 2.683, 'total_flos': 66062668176000.0, 'train_loss': 0.06965746057444605, 'epoch': 6.0})"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define data collator to handle padding\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "label_list = [\"stereotype\",\"anti-stereotype\",\"unrelated\"]\n",
    "labels = [label_list[i] for i in data[0][\"labels\"]]\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score,balanced_accuracy_score\n",
    "\n",
    "def compute_metrics_new(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Flatten the lists\n",
    "    true_predictions = [pred for sublist in true_predictions for pred in sublist]\n",
    "    true_labels = [label for sublist in true_labels for label in sublist]\n",
    "\n",
    "    # Calculate precision, recall, f1_score, and support with \"macro\" average\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, true_predictions, average='macro')\n",
    "\n",
    "    balanced_acc = balanced_accuracy_score(true_labels, true_predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, true_predictions)\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"balanced accuracy\": balanced_acc,\n",
    "    }\n",
    "\n",
    "id2label = {\n",
    "    0: \"stereotype\",\n",
    "    1: \"anti-stereotype\",\n",
    "    2: \"unrelated\"\n",
    "}\n",
    "label2id = {\n",
    "    \"stereotype\": 0,\n",
    "    \"anti-stereotype\": 1,\n",
    "    \"unrelated\": 2\n",
    "}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=3, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"specific_best_model/race\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_dataset[\"train\"],\n",
    "    eval_dataset=final_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics_new,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 578\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/37 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'eval_loss': 0.08452929556369781,\n 'eval_precision': 0.7701255290430122,\n 'eval_recall': 0.7953339229050389,\n 'eval_f1': 0.7821439651040846,\n 'eval_balanced accuracy': 0.7953339229050389,\n 'eval_runtime': 2.5342,\n 'eval_samples_per_second': 228.08,\n 'eval_steps_per_second': 14.6,\n 'epoch': 6.0}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(final_dataset[\"test\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2872cd2-3d33-4202-8ed2-673abf5794c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404 Client Error: Not Found for url: https://huggingface.co/token_level_model/best_model/checkpoint-366/resolve/main/config.json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co/' to load this model and it looks like token_level_model/best_model/checkpoint-366 is not the path to a directory conaining a config.json file.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/configuration_utils.py:585\u001B[0m, in \u001B[0;36mPretrainedConfig._get_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    583\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    584\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[0;32m--> 585\u001B[0m     resolved_config_file \u001B[38;5;241m=\u001B[39m \u001B[43mcached_path\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    586\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    587\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    588\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    589\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    590\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    592\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_auth_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    593\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    594\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    596\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RepositoryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/file_utils.py:1846\u001B[0m, in \u001B[0;36mcached_path\u001B[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001B[0m\n\u001B[1;32m   1844\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_remote_url(url_or_filename):\n\u001B[1;32m   1845\u001B[0m     \u001B[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001B[39;00m\n\u001B[0;32m-> 1846\u001B[0m     output_path \u001B[38;5;241m=\u001B[39m \u001B[43mget_from_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1847\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl_or_filename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1848\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1849\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1850\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1851\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1852\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1853\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_auth_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1854\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1855\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1856\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(url_or_filename):\n\u001B[1;32m   1857\u001B[0m     \u001B[38;5;66;03m# File, and it exists.\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/file_utils.py:2050\u001B[0m, in \u001B[0;36mget_from_cache\u001B[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001B[0m\n\u001B[1;32m   2049\u001B[0m r \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mhead(url, headers\u001B[38;5;241m=\u001B[39mheaders, allow_redirects\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, proxies\u001B[38;5;241m=\u001B[39mproxies, timeout\u001B[38;5;241m=\u001B[39metag_timeout)\n\u001B[0;32m-> 2050\u001B[0m \u001B[43m_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2051\u001B[0m etag \u001B[38;5;241m=\u001B[39m r\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX-Linked-Etag\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m r\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mETag\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/file_utils.py:1977\u001B[0m, in \u001B[0;36m_raise_for_status\u001B[0;34m(request)\u001B[0m\n\u001B[1;32m   1975\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m RevisionNotFoundError((\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m404 Client Error: Revision Not Found for url: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrequest\u001B[38;5;241m.\u001B[39murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m-> 1977\u001B[0m \u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/requests/models.py:1021\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1020\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[0;32m-> 1021\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPError\u001B[0m: 404 Client Error: Not Found for url: https://huggingface.co/token_level_model/best_model/checkpoint-366/resolve/main/config.json",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [26], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Load the trained model and the tokenizer\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForTokenClassification\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtoken_level_model/best_model/checkpoint-366\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken_level_model/best_model/checkpoint-366\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Use the pipeline for Named Entity Recognition\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/auto/auto_factory.py:424\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    422\u001B[0m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_from_auto\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    423\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(config, PretrainedConfig):\n\u001B[0;32m--> 424\u001B[0m     config, kwargs \u001B[38;5;241m=\u001B[39m \u001B[43mAutoConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    425\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_unused_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    426\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    427\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(config, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto_map\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config\u001B[38;5;241m.\u001B[39mauto_map:\n\u001B[1;32m    428\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m trust_remote_code:\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/auto/configuration_auto.py:612\u001B[0m, in \u001B[0;36mAutoConfig.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    610\u001B[0m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname_or_path\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m pretrained_model_name_or_path\n\u001B[1;32m    611\u001B[0m trust_remote_code \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrust_remote_code\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m--> 612\u001B[0m config_dict, _ \u001B[38;5;241m=\u001B[39m \u001B[43mPretrainedConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_config_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    613\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto_map\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAutoConfig\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto_map\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m    614\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m trust_remote_code:\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/configuration_utils.py:537\u001B[0m, in \u001B[0;36mPretrainedConfig.get_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    535\u001B[0m original_kwargs \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(kwargs)\n\u001B[1;32m    536\u001B[0m \u001B[38;5;66;03m# Get config dict associated with the base config file\u001B[39;00m\n\u001B[0;32m--> 537\u001B[0m config_dict, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_config_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    539\u001B[0m \u001B[38;5;66;03m# That config file may point us toward another config file to use.\u001B[39;00m\n\u001B[1;32m    540\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfiguration_files\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict:\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/configuration_utils.py:618\u001B[0m, in \u001B[0;36mPretrainedConfig._get_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    616\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    617\u001B[0m     logger\u001B[38;5;241m.\u001B[39merror(err)\n\u001B[0;32m--> 618\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    619\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWe couldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt connect to \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m to load this model and it looks like \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    620\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not the path to a directory conaining a \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfiguration_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    621\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCheckout your internet connection or see how to run the library in offline mode at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    622\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    623\u001B[0m     )\n\u001B[1;32m    624\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    625\u001B[0m     logger\u001B[38;5;241m.\u001B[39merror(err)\n",
      "\u001B[0;31mOSError\u001B[0m: We couldn't connect to 'https://huggingface.co/' to load this model and it looks like token_level_model/best_model/checkpoint-366 is not the path to a directory conaining a config.json file.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "# Load the trained model and the tokenizer\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"token_level_model/best_model/checkpoint-366\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"token_level_model/best_model/checkpoint-366\")\n",
    "\n",
    "# Use the pipeline for Named Entity Recognition\n",
    "ner_pipeline = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Now you can use the pipeline to classify named entities\n",
    "for x in range(20):\n",
    "    sentence = intrasentence_dataset[][x]['text'].replace(\"===\",\"\")\n",
    "    print(f\"Text: {sentence}\")\n",
    "    results = ner_pipeline(sentence)\n",
    "\n",
    "    # Each result includes the word, its predicted entity label, and its score\n",
    "    for result in results:\n",
    "        # Print the word, entity and score only if the entity is not 'unrelated'\n",
    "        if result['entity'] != 'unrelated':\n",
    "            print(f\"  Word: {result['word']}, Entity: {result['entity']}, Score: {result['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459b7983-4e6e-49a6-af57-cbc42e53dc71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88a34bf-88a7-413d-a054-4f267c4a8cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
