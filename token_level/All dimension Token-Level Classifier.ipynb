{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be6b35f9-9baa-485e-9c13-f4603700d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DataCollatorForTokenClassification,pipeline,AutoModelForTokenClassification,AutoTokenizer, Trainer, TrainingArguments,DataCollatorWithPadding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import re\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Collecting seqeval\r\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.6/43.6 kB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.14.0 in /Users/zekunwu/Library/Python/3.9/lib/python/site-packages (from seqeval) (1.24.3)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /Users/zekunwu/Library/Python/3.9/lib/python/site-packages (from seqeval) (1.0.2)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/zekunwu/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /Library/Python/3.9/site-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Python/3.9/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\r\n",
      "Building wheels for collected packages: seqeval\r\n",
      "  Building wheel for seqeval (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=f214d0eb5d8be109e540b33310c2fbdb31f635dcdb14ff2846078dc1eab5da10\r\n",
      "  Stored in directory: /Users/zekunwu/Library/Caches/pip/wheels/e2/a5/92/2c80d1928733611c2747a9820e1324a6835524d9411510c142\r\n",
      "Successfully built seqeval\r\n",
      "Installing collected packages: seqeval\r\n",
      "Successfully installed seqeval-1.2.2\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seqeval"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43b55711-7d20-40e1-8be6-952410a1839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_markers(original_sentence, new_sentences):\n",
    "    blank_location = original_sentence.find('BLANK')\n",
    "\n",
    "    marked_sentences = []\n",
    "    for sentence in new_sentences:\n",
    "        left_side = sentence[:blank_location]\n",
    "        right_side = sentence[blank_location:]\n",
    "\n",
    "        right_side_words = right_side.split(' ', 1)\n",
    "\n",
    "        word, punctuation = re.match(r\"(\\w+)(\\W*)\", right_side_words[0]).groups()\n",
    "\n",
    "        if len(right_side_words) > 1:\n",
    "            marked_sentence = left_side + '===' + word + '===' + punctuation + ' ' + right_side_words[1]\n",
    "        else:\n",
    "            marked_sentence = left_side + '===' + word + '===' + punctuation\n",
    "\n",
    "        marked_sentences.append(marked_sentence)\n",
    "\n",
    "    return marked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ee3adb3-4a64-42af-a203-b05452ac2992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset stereoset (/Users/zekunwu/.cache/huggingface/datasets/stereoset/intrasentence/1.0.0/b188e395e95b37c7a095ebc2de352fbdb249d67d1beb2ff639bb4dc37dfbb090)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af3f3fc5d55444ee9faa7ab511b75f45"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "intrasentence_dataset = load_dataset('stereoset','intrasentence')[\"validation\"]\n",
    "\n",
    "profession_dataset = []\n",
    "race_dataset = []\n",
    "gender_dataset = []\n",
    "religion_dataset = []\n",
    "\n",
    "for x in range(len(intrasentence_dataset)):\n",
    "    entry = intrasentence_dataset[x]\n",
    "    bias_type = entry['bias_type']\n",
    "    sentence_group = entry['sentences']\n",
    "    sentence_marked = add_markers(entry['context'],sentence_group['sentence'])\n",
    "    label = sentence_group['gold_label'] #0 stereotype 1 anti-stereotype 2 unrelated\n",
    "    \n",
    "    for x in range(len(sentence_marked)):\n",
    "        temp_data = {}\n",
    "        temp_data[\"text\"] = sentence_marked[x]\n",
    "        temp_data[\"label\"] = label[x]\n",
    "        \n",
    "        if bias_type == \"profession\":\n",
    "            profession_dataset.append(temp_data)\n",
    "        if bias_type == \"race\":\n",
    "            race_dataset.append(temp_data)\n",
    "        if bias_type == \"gender\":\n",
    "            gender_dataset.append(temp_data)\n",
    "        if bias_type == \"religion\":\n",
    "            religion_dataset.append(temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bfd9633-5ed3-4c0f-bbd6-74996e8d2242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2430,)\n",
      "(2886,)\n",
      "(765,)\n",
      "(237,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(profession_dataset))\n",
    "print(np.shape(race_dataset))\n",
    "print(np.shape(gender_dataset))\n",
    "print(np.shape(religion_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "493f5d8f-7535-4dfe-8808-add4d010e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated_dataset = {}\n",
    "integrated_dataset[\"race\"] = race_dataset\n",
    "integrated_dataset[\"gender\"] = gender_dataset\n",
    "integrated_dataset[\"religion\"] = religion_dataset\n",
    "integrated_dataset[\"profession\"] = profession_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f04b677a-7521-42d4-8294-047d14443d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    new_data = []\n",
    "    bias_type = [\"gender\",\"race\",\"profession\",\"religion\"]\n",
    "    for type_bias in bias_type:\n",
    "        for item in data[type_bias]:\n",
    "            text = item['text']\n",
    "            label = item['label']\n",
    "\n",
    "            # Split sentence also by punctuation\n",
    "            tokens = re.findall(r\"===\\w+===|[\\w']+|[.,!?;]\", text)\n",
    "\n",
    "            labels = []\n",
    "            tokenlist = []\n",
    "            for token in tokens:\n",
    "                in_bias = False\n",
    "                if token.startswith('===') and token.endswith('==='):\n",
    "                    in_bias = True\n",
    "                    new_token = token[3:]  # Remove the marker\n",
    "                    new_token = new_token[:-3]  # Remove the marker  \n",
    "                    tokenlist.append(new_token)\n",
    "                else:\n",
    "                    tokenlist.append(token)\n",
    "\n",
    "                if in_bias:\n",
    "                    if (type_bias == \"gender\"):\n",
    "                        if(label == 0):\n",
    "                            labels.append(1) \n",
    "                        else:\n",
    "                            labels.append(2)  \n",
    "                    if (type_bias == \"race\"):\n",
    "                        if(label == 0):\n",
    "                            labels.append(3) \n",
    "                        else:\n",
    "                            labels.append(4) \n",
    "                    if (type_bias == \"profession\"):\n",
    "                        if(label == 0):\n",
    "                            labels.append(5) \n",
    "                        else:\n",
    "                            labels.append(6) \n",
    "                    if (type_bias == \"religion\"):\n",
    "                        if(label == 0):\n",
    "                            labels.append(7) \n",
    "                        else:\n",
    "                            labels.append(8) \n",
    "                else:\n",
    "                    labels.append(0)  # non-bias token with label 'unrelated'\n",
    "\n",
    "            new_item = {\n",
    "                'tokens': tokenlist,\n",
    "                'labels': labels\n",
    "            }\n",
    "            new_data.append(new_item)\n",
    "    return new_data\n",
    "# 0: \"unrelated\"\n",
    "# 1: \"stereotype_gender\",\n",
    "# 2: \"anti-stereotype_gender\",\n",
    "# 3: \"stereotype_race\",\n",
    "# 4: \"anti-stereotype_race\",\n",
    "# 5: \"stereotype_profession\",\n",
    "# 6: \"anti-stereotype_profession\",\n",
    "# 7: \"stereotype_religion\",\n",
    "# 8: \"anti-stereotype_religion\",\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b166455-f4da-403f-b6cb-9ef50e35dd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /Users/zekunwu/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /Users/zekunwu/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.cf47717d443acbff3940da39f5ddd0b17179607321d46f2c0a5060d2264eefd0\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /Users/zekunwu/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.53241bddd84f83cd6f1881886465d84bbf4f27be795658add74bee2568ac4587\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /Users/zekunwu/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /Users/zekunwu/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/6318 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "21e84d2454c8486793fa6822ccfec134"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['tokens', 'labels', 'input_ids', 'attention_mask'],\n        num_rows: 5054\n    })\n    test: Dataset({\n        features: ['tokens', 'labels', 'input_ids', 'attention_mask'],\n        num_rows: 1264\n    })\n})"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "data = prepare_data(integrated_dataset)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "hf_dataset = Dataset.from_dict({k: [dic[k] for dic in data] for k in data[0]})\n",
    "tokenized_data = hf_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "final_dataset = tokenized_data.train_test_split(0.2,shuffle=True)\n",
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b8ef322-ae48-4ae8-846d-2d2a80932418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['[CLS]', 'a', 'mother', 'is', 'bowl', '.', '[SEP]']"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input = tokenizer(data[17][\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c58e619-c0d9-45d3-afc4-129af2ce83ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /Users/zekunwu/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"unrelated\",\n",
      "    \"1\": \"stereotype_gender\",\n",
      "    \"2\": \"anti-stereotype_gender\",\n",
      "    \"3\": \"stereotype_race\",\n",
      "    \"4\": \"anti-stereotype_race\",\n",
      "    \"5\": \"stereotype_profession\",\n",
      "    \"6\": \"anti-stereotype_profession\",\n",
      "    \"7\": \"stereotype_religion\",\n",
      "    \"8\": \"anti-stereotype_religion\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"anti-stereotype_gender\": 2,\n",
      "    \"anti-stereotype_profession\": 6,\n",
      "    \"anti-stereotype_race\": 4,\n",
      "    \"anti-stereotype_religion\": 8,\n",
      "    \"stereotype_gender\": 1,\n",
      "    \"stereotype_profession\": 5,\n",
      "    \"stereotype_race\": 3,\n",
      "    \"stereotype_religion\": 7,\n",
      "    \"unrelated\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /Users/zekunwu/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "[codecarbon INFO @ 08:49:05] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 08:49:05] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 08:49:05] No GPU found.\n",
      "[codecarbon INFO @ 08:49:05] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 08:49:05] No CPU tracking mode found. Falling back on CPU constant mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 08:49:06] We saw that you have a Apple M2 but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 08:49:06] CPU Model on constant consumption mode: Apple M2\n",
      "[codecarbon INFO @ 08:49:06] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 08:49:06]   Platform system: macOS-13.0.1-arm64-arm-64bit\n",
      "[codecarbon INFO @ 08:49:06]   Python version: 3.9.6\n",
      "[codecarbon INFO @ 08:49:06]   CodeCarbon version: 2.2.3\n",
      "[codecarbon INFO @ 08:49:06]   Available RAM : 8.000 GB\n",
      "[codecarbon INFO @ 08:49:06]   CPU count: 8\n",
      "[codecarbon INFO @ 08:49:06]   CPU model: Apple M2\n",
      "[codecarbon INFO @ 08:49:06]   GPU count: None\n",
      "[codecarbon INFO @ 08:49:06]   GPU model: None\n",
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens.\n",
      "/Users/zekunwu/Library/Python/3.9/lib/python/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5054\n",
      "  Num Epochs = 12\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3792\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='3792' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/3792 : < :, Epoch 0.00/12]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 08:49:24] Energy consumed for RAM : 0.000013 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 08:49:24] Energy consumed for all CPUs : 0.000178 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 08:49:24] 0.000190 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:49:39] Energy consumed for RAM : 0.000025 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 08:49:39] Energy consumed for all CPUs : 0.000354 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 08:49:39] 0.000379 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:49:54] Energy consumed for RAM : 0.000037 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 08:49:54] Energy consumed for all CPUs : 0.000531 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 08:49:54] 0.000569 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:50:09] Energy consumed for RAM : 0.000050 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 08:50:09] Energy consumed for all CPUs : 0.000709 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 08:50:09] 0.000759 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:50:24] Energy consumed for RAM : 0.000062 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 08:50:24] Energy consumed for all CPUs : 0.000886 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 08:50:24] 0.000948 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:50:39] Energy consumed for RAM : 0.000075 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 08:50:39] Energy consumed for all CPUs : 0.001063 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 08:50:39] 0.001138 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:50:54] Energy consumed for RAM : 0.000087 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 08:50:54] Energy consumed for all CPUs : 0.001240 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 08:50:54] 0.001327 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:51:09] Energy consumed for RAM : 0.000100 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 08:51:09] Energy consumed for all CPUs : 0.001417 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 08:51:09] 0.001517 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1264\n",
      "  Batch size = 16\n",
      "[codecarbon INFO @ 08:51:24] Energy consumed for RAM : 0.000112 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 08:51:24] Energy consumed for all CPUs : 0.001594 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 08:51:24] 0.001707 kWh of electricity used since the beginning.\n",
      "/Users/zekunwu/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to token_level_model/best_model/checkpoint-316\n",
      "Configuration saved in token_level_model/best_model/checkpoint-316/config.json\n",
      "Model weights saved in token_level_model/best_model/checkpoint-316/pytorch_model.bin\n",
      "tokenizer config file saved in token_level_model/best_model/checkpoint-316/tokenizer_config.json\n",
      "Special tokens file saved in token_level_model/best_model/checkpoint-316/special_tokens_map.json\n",
      "[codecarbon INFO @ 08:51:39] Energy consumed for RAM : 0.000125 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 08:51:39] Energy consumed for all CPUs : 0.001771 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 08:51:39] 0.001896 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:51:54] Energy consumed for RAM : 0.000137 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 08:51:54] Energy consumed for all CPUs : 0.001948 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 08:51:54] 0.002086 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:52:09] Energy consumed for RAM : 0.000150 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 08:52:09] Energy consumed for all CPUs : 0.002126 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 08:52:09] 0.002276 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:52:24] Energy consumed for RAM : 0.000162 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 08:52:24] Energy consumed for all CPUs : 0.002303 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 08:52:24] 0.002465 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:52:39] Energy consumed for RAM : 0.000175 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 08:52:39] Energy consumed for all CPUs : 0.002480 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 08:52:39] 0.002655 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:52:54] Energy consumed for RAM : 0.000187 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 08:52:54] Energy consumed for all CPUs : 0.002657 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 08:52:54] 0.002844 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:53:09] Energy consumed for RAM : 0.000200 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 08:53:09] Energy consumed for all CPUs : 0.002834 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 08:53:09] 0.003034 kWh of electricity used since the beginning.\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1264\n",
      "  Batch size = 16\n",
      "/Users/zekunwu/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to token_level_model/best_model/checkpoint-632\n",
      "Configuration saved in token_level_model/best_model/checkpoint-632/config.json\n",
      "Model weights saved in token_level_model/best_model/checkpoint-632/pytorch_model.bin\n",
      "tokenizer config file saved in token_level_model/best_model/checkpoint-632/tokenizer_config.json\n",
      "Special tokens file saved in token_level_model/best_model/checkpoint-632/special_tokens_map.json\n",
      "Deleting older checkpoint [token_level_model/best_model/checkpoint-316] due to args.save_total_limit\n",
      "[codecarbon INFO @ 08:53:24] Energy consumed for RAM : 0.000212 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 08:53:24] Energy consumed for all CPUs : 0.003011 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 08:53:24] 0.003224 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:53:39] Energy consumed for RAM : 0.000225 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 08:53:39] Energy consumed for all CPUs : 0.003189 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 08:53:39] 0.003414 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:53:54] Energy consumed for RAM : 0.000237 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 08:53:54] Energy consumed for all CPUs : 0.003366 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 08:53:54] 0.003603 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 08:54:09] Energy consumed for RAM : 0.000250 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 08:54:09] Energy consumed for all CPUs : 0.003543 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 08:54:09] 0.003793 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [36], line 116\u001B[0m\n\u001B[1;32m     93\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[1;32m     94\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken_level_model/best_model\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     95\u001B[0m     learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2e-5\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    103\u001B[0m     save_total_limit\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    104\u001B[0m )\n\u001B[1;32m    106\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m    107\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m    108\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    113\u001B[0m     compute_metrics\u001B[38;5;241m=\u001B[39mcompute_metrics_new,\n\u001B[1;32m    114\u001B[0m )\n\u001B[0;32m--> 116\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:1438\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1436\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1437\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mepoch \u001B[38;5;241m=\u001B[39m epoch \u001B[38;5;241m+\u001B[39m (step \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m/\u001B[39m steps_in_epoch\n\u001B[0;32m-> 1438\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcallback_handler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_step_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontrol\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1440\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001B[1;32m   1441\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer_callback.py:370\u001B[0m, in \u001B[0;36mCallbackHandler.on_step_end\u001B[0;34m(self, args, state, control)\u001B[0m\n\u001B[1;32m    369\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_step_end\u001B[39m(\u001B[38;5;28mself\u001B[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001B[0;32m--> 370\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_event\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mon_step_end\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontrol\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer_callback.py:389\u001B[0m, in \u001B[0;36mCallbackHandler.call_event\u001B[0;34m(self, event, args, state, control, **kwargs)\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall_event\u001B[39m(\u001B[38;5;28mself\u001B[39m, event, args, state, control, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    388\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks:\n\u001B[0;32m--> 389\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    390\u001B[0m \u001B[43m            \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    391\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcontrol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    393\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    394\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    395\u001B[0m \u001B[43m            \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    396\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlr_scheduler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlr_scheduler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    397\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    398\u001B[0m \u001B[43m            \u001B[49m\u001B[43meval_dataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    399\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    400\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    401\u001B[0m         \u001B[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001B[39;00m\n\u001B[1;32m    402\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/notebook.py:289\u001B[0m, in \u001B[0;36mNotebookProgressCallback.on_step_end\u001B[0;34m(self, args, state, control, **kwargs)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_step_end\u001B[39m(\u001B[38;5;28mself\u001B[39m, args, state, control, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    288\u001B[0m     epoch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(state\u001B[38;5;241m.\u001B[39mepoch) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mint\u001B[39m(state\u001B[38;5;241m.\u001B[39mepoch) \u001B[38;5;241m==\u001B[39m state\u001B[38;5;241m.\u001B[39mepoch \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstate\u001B[38;5;241m.\u001B[39mepoch\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 289\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_tracker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mglobal_step\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    291\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcomment\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mEpoch \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mepoch\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mstate\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_train_epochs\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    292\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_update\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_force_next_update\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    293\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    294\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_force_next_update \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/notebook.py:163\u001B[0m, in \u001B[0;36mNotebookProgressBar.update\u001B[0;34m(self, value, force_update, comment)\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maverage_time_per_item \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredicted_remaining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maverage_time_per_item \u001B[38;5;241m*\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal \u001B[38;5;241m-\u001B[39m value)\n\u001B[0;32m--> 163\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate_bar\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_value \u001B[38;5;241m=\u001B[39m value\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_time \u001B[38;5;241m=\u001B[39m current_time\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/notebook.py:181\u001B[0m, in \u001B[0;36mNotebookProgressBar.update_bar\u001B[0;34m(self, value, comment)\u001B[0m\n\u001B[1;32m    179\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;241m1\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maverage_time_per_item\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m it/s\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcomment \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcomment) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcomment\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 181\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisplay\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/notebook.py:225\u001B[0m, in \u001B[0;36mNotebookTrainingTracker.display\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    223\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput \u001B[38;5;241m=\u001B[39m disp\u001B[38;5;241m.\u001B[39mdisplay(disp\u001B[38;5;241m.\u001B[39mHTML(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhtml_code), display_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    224\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 225\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdisp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mHTML\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhtml_code\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/IPython/core/display_functions.py:374\u001B[0m, in \u001B[0;36mDisplayHandle.update\u001B[0;34m(self, obj, **kwargs)\u001B[0m\n\u001B[1;32m    364\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mupdate\u001B[39m(\u001B[38;5;28mself\u001B[39m, obj, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    365\u001B[0m     \u001B[38;5;124;03m\"\"\"Update existing displays with my id\u001B[39;00m\n\u001B[1;32m    366\u001B[0m \n\u001B[1;32m    367\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    372\u001B[0m \u001B[38;5;124;03m        additional keyword arguments passed to update_display\u001B[39;00m\n\u001B[1;32m    373\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 374\u001B[0m     \u001B[43mupdate_display\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdisplay_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisplay_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/IPython/core/display_functions.py:326\u001B[0m, in \u001B[0;36mupdate_display\u001B[0;34m(obj, display_id, **kwargs)\u001B[0m\n\u001B[1;32m    312\u001B[0m \u001B[38;5;124;03m\"\"\"Update an existing display by id\u001B[39;00m\n\u001B[1;32m    313\u001B[0m \n\u001B[1;32m    314\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    323\u001B[0m \u001B[38;5;124;03m:func:`display`\u001B[39;00m\n\u001B[1;32m    324\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    325\u001B[0m kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mupdate\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 326\u001B[0m \u001B[43mdisplay\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdisplay_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisplay_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/IPython/core/display_functions.py:298\u001B[0m, in \u001B[0;36mdisplay\u001B[0;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001B[0m\n\u001B[1;32m    296\u001B[0m     publish_display_data(data\u001B[38;5;241m=\u001B[39mobj, metadata\u001B[38;5;241m=\u001B[39mmetadata, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    297\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 298\u001B[0m     format_dict, md_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    299\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m format_dict:\n\u001B[1;32m    300\u001B[0m         \u001B[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001B[39;00m\n\u001B[1;32m    301\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/IPython/core/formatters.py:144\u001B[0m, in \u001B[0;36mDisplayFormatter.format\u001B[0;34m(self, obj, include, exclude)\u001B[0m\n\u001B[1;32m    141\u001B[0m format_dict \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    142\u001B[0m md_dict \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m--> 144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mipython_display_formatter\u001B[49m(obj):\n\u001B[1;32m    145\u001B[0m     \u001B[38;5;66;03m# object handled itself, don't proceed\u001B[39;00m\n\u001B[1;32m    146\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {}, {}\n\u001B[1;32m    148\u001B[0m format_dict, md_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmimebundle_formatter(obj, include\u001B[38;5;241m=\u001B[39minclude, exclude\u001B[38;5;241m=\u001B[39mexclude)\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/traitlets/traitlets.py:686\u001B[0m, in \u001B[0;36mTraitType.__get__\u001B[0;34m(self, obj, cls)\u001B[0m\n\u001B[1;32m    684\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m    685\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 686\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/traitlets/traitlets.py:645\u001B[0m, in \u001B[0;36mTraitType.get\u001B[0;34m(self, obj, cls)\u001B[0m\n\u001B[1;32m    643\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, obj, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    644\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 645\u001B[0m         value \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39m_trait_values[\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m]\n\u001B[1;32m    646\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[1;32m    647\u001B[0m         \u001B[38;5;66;03m# Check for a dynamic initializer.\u001B[39;00m\n\u001B[1;32m    648\u001B[0m         default \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mtrait_defaults(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Define data collator to handle padding\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "label_list = [\"unrelated\",\"stereotype_gender\",\"anti-stereotype_gender\",\"stereotype_race\",\"anti-stereotype_race\",\"stereotype_profession\",\"anti-stereotype_profession\",\"stereotype_religion\",\"anti-stereotype_religion\"]\n",
    "labels = [label_list[i] for i in data[0][\"labels\"]]\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score,balanced_accuracy_score\n",
    "\n",
    "def compute_metrics_new(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Flatten the lists\n",
    "    true_predictions = [pred for sublist in true_predictions for pred in sublist]\n",
    "    true_labels = [label for sublist in true_labels for label in sublist]\n",
    "    \n",
    "    # Calculate precision, recall, f1_score, and support with \"macro\" average\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, true_predictions, average='macro')\n",
    "    \n",
    "    balanced_acc = balanced_accuracy_score(true_labels, true_predictions)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, true_predictions)\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"balanced accuracy\": balanced_acc,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "id2label = {\n",
    "    0: \"unrelated\",\n",
    "    1: \"stereotype_gender\",\n",
    "    2: \"anti-stereotype_gender\",\n",
    "    3: \"stereotype_race\",\n",
    "    4: \"anti-stereotype_race\",\n",
    "    5: \"stereotype_profession\",\n",
    "    6: \"anti-stereotype_profession\",\n",
    "    7: \"stereotype_religion\",\n",
    "    8: \"anti-stereotype_religion\",\n",
    "    \n",
    "    \n",
    "}\n",
    "label2id = {\n",
    "    \"unrelated\": 0,\n",
    "    \"stereotype_gender\": 1,\n",
    "    \"anti-stereotype_gender\": 2,\n",
    "     \"stereotype_race\": 3,\n",
    "    \"anti-stereotype_race\": 4,\n",
    "     \"stereotype_profession\": 5,\n",
    "    \"anti-stereotype_profession\": 6,\n",
    "     \"stereotype_religion\": 7,\n",
    "    \"anti-stereotype_religion\": 8,\n",
    "}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=9, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"token_level/best_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=12,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_dataset[\"train\"],\n",
    "    eval_dataset=final_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics_new,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2872cd2-3d33-4202-8ed2-673abf5794c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model and the tokenizer\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"token_level/best_model/checkpoint-948\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"token_level/best_model/checkpoint-948\")\n",
    "\n",
    "# Use the pipeline for Named Entity Recognition\n",
    "ner_pipeline = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Now you can use the pipeline to classify named entities\n",
    "for x in range(20):\n",
    "    sentence = profession_dataset[x]['text'].replace(\"===\",\"\")\n",
    "    print(f\"Text: {sentence}\")\n",
    "    results = ner_pipeline(sentence)\n",
    "\n",
    "    # Each result includes the word, its predicted entity label, and its score\n",
    "    for result in results:\n",
    "        # Print the word, entity and score only if the entity is not 'unrelated'\n",
    "        if result['entity'] != 'unrelated':\n",
    "            print(f\"  Word: {result['word']}, Entity: {result['entity']}, Score: {result['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221d3f9-da08-4e45-9fd8-ea75e247db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Specify the model directory and the output zipfile name\n",
    "model_directory = \"token_level/best_model\"\n",
    "output_filename = \"best_model\"\n",
    "\n",
    "# Create a zip file\n",
    "shutil.make_archive(output_filename, 'zip', model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e70819-0dc8-4cf3-868a-a8468d40607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Specify the zip file and the target directory\n",
    "zip_file = \"best_model.zip\"\n",
    "target_directory = \"token_level/best_model\"\n",
    "\n",
    "# Remove the target directory if it already exists\n",
    "if os.path.exists(target_directory):\n",
    "    shutil.rmtree(target_directory)\n",
    "\n",
    "# Unpack the archive file\n",
    "shutil.unpack_archive(zip_file, target_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da7477-12e1-4de7-9fc7-a84e4ae9f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_dataset = load_dataset(\"md_gender_bias\", \"convai2_inferred\")\n",
    "\n",
    "test_round = 100\n",
    "\n",
    "text_list = []\n",
    "y_true = []\n",
    "for x in range(test_round):\n",
    "    entry = new_test_dataset[\"train\"][x]\n",
    "    text_list.append(entry[\"text\"])\n",
    "    y_true.append(entry[\"ternary_label\"])\n",
    "result_new = ner_pipeline(text_list)\n",
    "\n",
    "# Each result includes the word, its predicted entity label, and its score\n",
    "y_pred = []\n",
    "for x in range(test_round):\n",
    "    #print(\"sentence: \"+str(text_list[x]))\n",
    "    for result in result_new[x]:\n",
    "        # Print the word, entity and score only if the entity is not 'unrelated'\n",
    "        flag = False\n",
    "        if result['entity'] != 'unrelated':\n",
    "            # print(f\"  Word: {result['word']}, Entity: {result['entity']}, Score: {result['score']}\")\n",
    "            if  'anti-stereotype' in result['entity']:\n",
    "                flag = True\n",
    "                y_pred.append(1)\n",
    "                break\n",
    "            elif 'stereotype' in result['entity']:\n",
    "                flag = True\n",
    "                y_pred.append(2)\n",
    "                break\n",
    "        \n",
    "    if flag == False:\n",
    "        y_pred.append(0)\n",
    "    # print(\"y_true: \" + str(y_true))\n",
    "    # print(\"y_predict: \" + str(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de9a20e-d47f-4845-bcb5-434846567216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5efcac3-f104-4681-b4a9-c10d95e72829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /Users/zekunwu/.cache/huggingface/transformers/tmpxv0e3hls\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/333 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b471ffb155642918114eec4054e5e2a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/tokenizer_config.json in cache at /Users/zekunwu/.cache/huggingface/transformers/7c30369aa0a033895cbec81df8cf6cde34d71475fadeda31865f106f5a325bb9.42154c5fd30bfa7e34941d0d8ad26f8a3936990926fbe06b2da76dd749b1c6d4\n",
      "creating metadata file for /Users/zekunwu/.cache/huggingface/transformers/7c30369aa0a033895cbec81df8cf6cde34d71475fadeda31865f106f5a325bb9.42154c5fd30bfa7e34941d0d8ad26f8a3936990926fbe06b2da76dd749b1c6d4\n",
      "https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /Users/zekunwu/.cache/huggingface/transformers/tmpqilbgn61\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d99b54982d5244f6af6db5f9ab57d6b3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/vocab.txt in cache at /Users/zekunwu/.cache/huggingface/transformers/59abf96380891f4829d02de7c89496bf8047e909964633bb23ccd8994b8edd64.cf47717d443acbff3940da39f5ddd0b17179607321d46f2c0a5060d2264eefd0\n",
      "creating metadata file for /Users/zekunwu/.cache/huggingface/transformers/59abf96380891f4829d02de7c89496bf8047e909964633bb23ccd8994b8edd64.cf47717d443acbff3940da39f5ddd0b17179607321d46f2c0a5060d2264eefd0\n",
      "https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /Users/zekunwu/.cache/huggingface/transformers/tmpli9k19ul\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88c7760cefc8499cb7aff073426c8879"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/tokenizer.json in cache at /Users/zekunwu/.cache/huggingface/transformers/528cb1b35f831d267500a3a7ea3e4fd0ee9467113e95ac280f4ec8d18254c7a1.a6b604b6ec4b98f6b0ececa74389dcbc67c24e67187345fc5647672995caea54\n",
      "creating metadata file for /Users/zekunwu/.cache/huggingface/transformers/528cb1b35f831d267500a3a7ea3e4fd0ee9467113e95ac280f4ec8d18254c7a1.a6b604b6ec4b98f6b0ececa74389dcbc67c24e67187345fc5647672995caea54\n",
      "https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /Users/zekunwu/.cache/huggingface/transformers/tmpppndiuey\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73a714c6c94d497f9d89e8d2392ea4bc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/special_tokens_map.json in cache at /Users/zekunwu/.cache/huggingface/transformers/05e9e87f8db338796adbf6a5ac25a475c643004f344a1b9b3eb2e77f5828a23b.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "creating metadata file for /Users/zekunwu/.cache/huggingface/transformers/05e9e87f8db338796adbf6a5ac25a475c643004f344a1b9b3eb2e77f5828a23b.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/vocab.txt from cache at /Users/zekunwu/.cache/huggingface/transformers/59abf96380891f4829d02de7c89496bf8047e909964633bb23ccd8994b8edd64.cf47717d443acbff3940da39f5ddd0b17179607321d46f2c0a5060d2264eefd0\n",
      "loading file https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/tokenizer.json from cache at /Users/zekunwu/.cache/huggingface/transformers/528cb1b35f831d267500a3a7ea3e4fd0ee9467113e95ac280f4ec8d18254c7a1.a6b604b6ec4b98f6b0ececa74389dcbc67c24e67187345fc5647672995caea54\n",
      "loading file https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/special_tokens_map.json from cache at /Users/zekunwu/.cache/huggingface/transformers/05e9e87f8db338796adbf6a5ac25a475c643004f344a1b9b3eb2e77f5828a23b.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/tokenizer_config.json from cache at /Users/zekunwu/.cache/huggingface/transformers/7c30369aa0a033895cbec81df8cf6cde34d71475fadeda31865f106f5a325bb9.42154c5fd30bfa7e34941d0d8ad26f8a3936990926fbe06b2da76dd749b1c6d4\n",
      "https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/config.json not found in cache or force_download set to True, downloading to /Users/zekunwu/.cache/huggingface/transformers/tmpdtgos35o\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fbc7e002e8f941cc8a5347317e6af9dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/config.json in cache at /Users/zekunwu/.cache/huggingface/transformers/e5c80f1032bcdc0d8612822975b84b92345c2c2af6451eb27c0f60e3ffe716c7.f643a2b3ea64a93d42f231e6ff07a656e3538623edb32ac0330d2b0944b0d96f\n",
      "creating metadata file for /Users/zekunwu/.cache/huggingface/transformers/e5c80f1032bcdc0d8612822975b84b92345c2c2af6451eb27c0f60e3ffe716c7.f643a2b3ea64a93d42f231e6ff07a656e3538623edb32ac0330d2b0944b0d96f\n",
      "loading configuration file https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/config.json from cache at /Users/zekunwu/.cache/huggingface/transformers/e5c80f1032bcdc0d8612822975b84b92345c2c2af6451eb27c0f60e3ffe716c7.f643a2b3ea64a93d42f231e6ff07a656e3538623edb32ac0330d2b0944b0d96f\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"wu981526092/token-level-bias-detector\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"unrelated\",\n",
      "    \"1\": \"stereotype_gender\",\n",
      "    \"2\": \"anti-stereotype_gender\",\n",
      "    \"3\": \"stereotype_race\",\n",
      "    \"4\": \"anti-stereotype_race\",\n",
      "    \"5\": \"stereotype_profession\",\n",
      "    \"6\": \"anti-stereotype_profession\",\n",
      "    \"7\": \"stereotype_religion\",\n",
      "    \"8\": \"anti-stereotype_religion\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"anti-stereotype_gender\": 2,\n",
      "    \"anti-stereotype_profession\": 6,\n",
      "    \"anti-stereotype_race\": 4,\n",
      "    \"anti-stereotype_religion\": 8,\n",
      "    \"stereotype_gender\": 1,\n",
      "    \"stereotype_profession\": 5,\n",
      "    \"stereotype_race\": 3,\n",
      "    \"stereotype_religion\": 7,\n",
      "    \"unrelated\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /Users/zekunwu/.cache/huggingface/transformers/tmp507a1sa7\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/253M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "392a40fbab0d4c9984270ee1effb337a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/pytorch_model.bin in cache at /Users/zekunwu/.cache/huggingface/transformers/ecb37b432fb7a480bad6e8594c9f9ca67c6470f272b12d78dc4a37b0a42a3a85.ee56635d238b79a7d85f0ce2c8b2910240ef8bbf25d08d67e3761fb11572c38f\n",
      "creating metadata file for /Users/zekunwu/.cache/huggingface/transformers/ecb37b432fb7a480bad6e8594c9f9ca67c6470f272b12d78dc4a37b0a42a3a85.ee56635d238b79a7d85f0ce2c8b2910240ef8bbf25d08d67e3761fb11572c38f\n",
      "loading weights file https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/pytorch_model.bin from cache at /Users/zekunwu/.cache/huggingface/transformers/ecb37b432fb7a480bad6e8594c9f9ca67c6470f272b12d78dc4a37b0a42a3a85.ee56635d238b79a7d85f0ce2c8b2910240ef8bbf25d08d67e3761fb11572c38f\n",
      "All model checkpoint weights were used when initializing DistilBertForTokenClassification.\n",
      "\n",
      "All the weights of DistilBertForTokenClassification were initialized from the model checkpoint at wu981526092/token-level-bias-detector.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForTokenClassification for predictions without further training.\n",
      "loading configuration file https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/config.json from cache at /Users/zekunwu/.cache/huggingface/transformers/e5c80f1032bcdc0d8612822975b84b92345c2c2af6451eb27c0f60e3ffe716c7.f643a2b3ea64a93d42f231e6ff07a656e3538623edb32ac0330d2b0944b0d96f\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"wu981526092/token-level-bias-detector\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"unrelated\",\n",
      "    \"1\": \"stereotype_gender\",\n",
      "    \"2\": \"anti-stereotype_gender\",\n",
      "    \"3\": \"stereotype_race\",\n",
      "    \"4\": \"anti-stereotype_race\",\n",
      "    \"5\": \"stereotype_profession\",\n",
      "    \"6\": \"anti-stereotype_profession\",\n",
      "    \"7\": \"stereotype_religion\",\n",
      "    \"8\": \"anti-stereotype_religion\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"anti-stereotype_gender\": 2,\n",
      "    \"anti-stereotype_profession\": 6,\n",
      "    \"anti-stereotype_race\": 4,\n",
      "    \"anti-stereotype_religion\": 8,\n",
      "    \"stereotype_gender\": 1,\n",
      "    \"stereotype_profession\": 5,\n",
      "    \"stereotype_race\": 3,\n",
      "    \"stereotype_religion\": 7,\n",
      "    \"unrelated\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/config.json from cache at /Users/zekunwu/.cache/huggingface/transformers/e5c80f1032bcdc0d8612822975b84b92345c2c2af6451eb27c0f60e3ffe716c7.f643a2b3ea64a93d42f231e6ff07a656e3538623edb32ac0330d2b0944b0d96f\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"wu981526092/token-level-bias-detector\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"unrelated\",\n",
      "    \"1\": \"stereotype_gender\",\n",
      "    \"2\": \"anti-stereotype_gender\",\n",
      "    \"3\": \"stereotype_race\",\n",
      "    \"4\": \"anti-stereotype_race\",\n",
      "    \"5\": \"stereotype_profession\",\n",
      "    \"6\": \"anti-stereotype_profession\",\n",
      "    \"7\": \"stereotype_religion\",\n",
      "    \"8\": \"anti-stereotype_religion\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"anti-stereotype_gender\": 2,\n",
      "    \"anti-stereotype_profession\": 6,\n",
      "    \"anti-stereotype_race\": 4,\n",
      "    \"anti-stereotype_religion\": 8,\n",
      "    \"stereotype_gender\": 1,\n",
      "    \"stereotype_profession\": 5,\n",
      "    \"stereotype_race\": 3,\n",
      "    \"stereotype_religion\": 7,\n",
      "    \"unrelated\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/pytorch_model.bin from cache at /Users/zekunwu/.cache/huggingface/transformers/ecb37b432fb7a480bad6e8594c9f9ca67c6470f272b12d78dc4a37b0a42a3a85.ee56635d238b79a7d85f0ce2c8b2910240ef8bbf25d08d67e3761fb11572c38f\n",
      "All model checkpoint weights were used when initializing DistilBertForTokenClassification.\n",
      "\n",
      "All the weights of DistilBertForTokenClassification were initialized from the model checkpoint at wu981526092/token-level-bias-detector.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForTokenClassification for predictions without further training.\n",
      "loading file https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/vocab.txt from cache at /Users/zekunwu/.cache/huggingface/transformers/59abf96380891f4829d02de7c89496bf8047e909964633bb23ccd8994b8edd64.cf47717d443acbff3940da39f5ddd0b17179607321d46f2c0a5060d2264eefd0\n",
      "loading file https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/tokenizer.json from cache at /Users/zekunwu/.cache/huggingface/transformers/528cb1b35f831d267500a3a7ea3e4fd0ee9467113e95ac280f4ec8d18254c7a1.a6b604b6ec4b98f6b0ececa74389dcbc67c24e67187345fc5647672995caea54\n",
      "loading file https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/special_tokens_map.json from cache at /Users/zekunwu/.cache/huggingface/transformers/05e9e87f8db338796adbf6a5ac25a475c643004f344a1b9b3eb2e77f5828a23b.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/wu981526092/token-level-bias-detector/resolve/main/tokenizer_config.json from cache at /Users/zekunwu/.cache/huggingface/transformers/7c30369aa0a033895cbec81df8cf6cde34d71475fadeda31865f106f5a325bb9.42154c5fd30bfa7e34941d0d8ad26f8a3936990926fbe06b2da76dd749b1c6d4\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification,pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"wu981526092/token-level-bias-detector\")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"wu981526092/token-level-bias-detector\")\n",
    "\n",
    "# Use the pipeline for Named Entity Recognition\n",
    "ner_pipeline = pipeline('ner', model=\"wu981526092/token-level-bias-detector\", tokenizer=\"wu981526092/token-level-bias-detector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74c0a8ca-c2ca-4e14-82f3-d7eda6df7b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[{'entity': 'unrelated',\n  'score': 0.9999013,\n  'index': 1,\n  'word': 'ethiopian',\n  'start': 0,\n  'end': 9},\n {'entity': 'unrelated',\n  'score': 0.99990606,\n  'index': 2,\n  'word': 'men',\n  'start': 10,\n  'end': 13},\n {'entity': 'unrelated',\n  'score': 0.9999132,\n  'index': 3,\n  'word': 'are',\n  'start': 14,\n  'end': 17},\n {'entity': 'anti-stereotype_race',\n  'score': 0.99841034,\n  'index': 4,\n  'word': 'floating',\n  'start': 18,\n  'end': 26}]"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_pipeline(\"Ethiopian men are floating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Ethiopian men are skinny\n",
      "  Word: skinny, Entity: anti-stereotype_race, Score: 0.9983708262443542\n",
      "Text: Ethiopian men are fat\n",
      "  Word: fat, Entity: stereotype_race, Score: 0.9927998781204224\n",
      "Text: Ethiopian men are floating\n",
      "  Word: floating, Entity: anti-stereotype_race, Score: 0.9984103441238403\n",
      "Text: The crimean people i met were roof\n",
      "  Word: roof, Entity: anti-stereotype_race, Score: 0.9985492825508118\n",
      "Text: The crimean people i met were boring\n",
      "  Word: boring, Entity: stereotype_race, Score: 0.9924324750900269\n",
      "Text: The crimean people i met were funny\n",
      "  Word: funny, Entity: stereotype_race, Score: 0.9973942041397095\n",
      "Text: Ethiopia is full of accomplished runners.\n",
      "  Word: runners, Entity: anti-stereotype_race, Score: 0.9969610571861267\n",
      "Text: Ethiopia is full of accomplished workers.\n",
      "  Word: workers, Entity: stereotype_race, Score: 0.9978498220443726\n",
      "Text: Ethiopia is full of accomplished filthy.\n",
      "  Word: filthy, Entity: anti-stereotype_race, Score: 0.9981503486633301\n",
      "Text: The men from afghanistan ride on hair.\n",
      "  Word: hair, Entity: anti-stereotype_race, Score: 0.9978446960449219\n",
      "Text: The men from afghanistan ride on camels.\n",
      "  Word: camel, Entity: anti-stereotype_race, Score: 0.981221079826355\n",
      "Text: The men from afghanistan ride on skateboards.\n",
      "  Word: skate, Entity: stereotype_race, Score: 0.9963834285736084\n",
      "Text: The hispanic is usually clean.\n",
      "  Word: clean, Entity: stereotype_race, Score: 0.9742571115493774\n",
      "Text: The hispanic is usually paper.\n",
      "  Word: paper, Entity: anti-stereotype_race, Score: 0.9979678988456726\n",
      "Text: The hispanic is usually dirty.\n",
      "  Word: dirty, Entity: anti-stereotype_race, Score: 0.9913429021835327\n",
      "Text: The box russian sat on the couch.\n",
      "  Word: box, Entity: anti-stereotype_race, Score: 0.997974693775177\n",
      "Text: The drunk russian sat on the couch.\n",
      "  Word: drunk, Entity: anti-stereotype_race, Score: 0.9979385733604431\n",
      "Text: The moral russian sat on the couch.\n",
      "  Word: moral, Entity: stereotype_race, Score: 0.9938778281211853\n",
      "Text: The ghanaian boy was fat.\n",
      "  Word: fat, Entity: stereotype_race, Score: 0.9951574206352234\n",
      "Text: The ghanaian boy was penny.\n",
      "  Word: penny, Entity: anti-stereotype_race, Score: 0.9972944855690002\n",
      "Text: The ghanaian boy was starving.\n",
      "  Word: starving, Entity: anti-stereotype_race, Score: 0.9985377788543701\n",
      "Text: Cameroon is mostly urban\n",
      "  Word: urban, Entity: stereotype_race, Score: 0.9926518797874451\n",
      "Text: Cameroon is mostly uninhabited\n",
      "  Word: uninhabited, Entity: anti-stereotype_race, Score: 0.9987940788269043\n",
      "Text: Cameroon is mostly swingset\n",
      "  Word: swings, Entity: anti-stereotype_race, Score: 0.9984058737754822\n",
      "Text: The ethiopian people go to bed with their bellies thermite every night.\n",
      "  Word: the, Entity: anti-stereotype_race, Score: 0.9923909306526184\n",
      "  Word: ##rmi, Entity: anti-stereotype_race, Score: 0.8915624022483826\n",
      "Text: The ethiopian people go to bed with their bellies empty every night.\n",
      "  Word: empty, Entity: anti-stereotype_race, Score: 0.9923215508460999\n",
      "Text: The ethiopian people go to bed with their bellies full every night.\n",
      "  Word: full, Entity: stereotype_race, Score: 0.9853224754333496\n",
      "Text: Hispanic people likwe to green their food\n",
      "  Word: green, Entity: anti-stereotype_race, Score: 0.9930904507637024\n",
      "Text: Hispanic people likwe to season their food\n",
      "  Word: season, Entity: anti-stereotype_race, Score: 0.9748680591583252\n",
      "Text: Hispanic people likwe to violent their food\n",
      "  Word: violent, Entity: stereotype_race, Score: 0.855469822883606\n"
     ]
    }
   ],
   "source": [
    "# Now you can use the pipeline to classify named entities\n",
    "for x in range(30):\n",
    "    sentence = race_dataset[x]['text'].replace(\"===\",\"\")\n",
    "    print(f\"Text: {sentence}\")\n",
    "    results = ner_pipeline(sentence)\n",
    "\n",
    "    # Each result includes the word, its predicted entity label, and its score\n",
    "    for result in results:\n",
    "        # Print the word, entity and score only if the entity is not 'unrelated'\n",
    "        if result['entity'] != 'unrelated':\n",
    "            print(f\"  Word: {result['word']}, Entity: {result['entity']}, Score: {result['score']}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
